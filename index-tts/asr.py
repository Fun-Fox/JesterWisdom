import os
import warnings

from faster_whisper import WhisperModel

root_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

LOCAL_MODEL_PATH = os.path.join(root_dir, 'models', "large-v3")

class WhisperModelSingleton:
    _instance = None
    _model = None

    def __new__(cls, model_size="large-v3", device="cuda"):
        if cls._instance is None:
            cls._instance = super(WhisperModelSingleton, cls).__new__(cls)

            warnings.filterwarnings("ignore", category=FutureWarning)
            warnings.filterwarnings("ignore", category=UserWarning)
            # å¦‚æœæœ¬åœ°å­˜åœ¨æ¨¡å‹ï¼Œåˆ™ä»æœ¬åœ°åŠ è½½
            if os.path.exists(LOCAL_MODEL_PATH):
                print(f"ğŸ“¦ æ­£åœ¨ä»æœ¬åœ°åŠ è½½æ¨¡å‹: {LOCAL_MODEL_PATH}")
                cls._model = WhisperModel(
                    model_size_or_path=LOCAL_MODEL_PATH,
                    device=device,
                )
            else:
                print(f"ğŸŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹ï¼Œæ­£åœ¨ä»è¿œç¨‹ä¸‹è½½: {model_size}")
                cls._model = WhisperModel(
                    model_size_or_path=model_size,
                    device=device,
                )
        return cls._instance

    def transcribe(self, audio_path, **kwargs):
        """
        è°ƒç”¨ whisper æ¨¡å‹è¿›è¡Œè¯­éŸ³è¯†åˆ«
        :param audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        :param kwargs: å…¶ä»– transcribe å‚æ•°
        :return: segments, info
        """
        segments, info = self._model.transcribe(audio_path, **kwargs)
        segments = list(segments)
        return segments, info
def format_timestamp(seconds):
    hours = int(seconds // 3600)
    minutes = int((seconds % 3600) // 60)
    seconds = seconds % 60
    milliseconds = int((seconds - int(seconds)) * 1000)
    return f"{hours:02d}:{minutes:02d}:{int(seconds):02d},{milliseconds:03d}"


def get_whisper_model():
    """
    è·å–å•ä¾‹çš„ Whisper æ¨¡å‹
    """
    return WhisperModelSingleton()

def process_single_audio(whisper, audio_path, output_srt_path):
    """å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶"""
    print(f"ğŸ”Š å¤„ç†éŸ³é¢‘: {audio_path}")
    segments, _ = whisper.transcribe(audio_path, beam_size=7,language="en", condition_on_previous_text=False, word_timestamps=True)
    generate_srt(segments, output_srt_path)
    return output_srt_path

def generate_srt(segments, output_srt_path):
    with open(output_srt_path, "w", encoding="utf-8") as f:
        index = 1  # åºå·ä»1å¼€å§‹é€’å¢

        for segment in segments:
            start_time = segment.start
            end_time = segment.end
            text = segment.text.strip()

            # åˆ¤æ–­æ˜¯å¦æœ‰é€—å·
            if ',' in text:
                parts = [p.strip() for p in text.split(',') if p.strip()]
                if len(parts) == 0:
                    continue

                # å¹³å‡åˆ†é…æ—¶é—´
                duration = (end_time - start_time) / len(parts)
                times = []

                for i in range(len(parts)):
                    part_start = start_time + i * duration
                    part_end = start_time + (i + 1) * duration
                    times.append((part_start, part_end))

                # å†™å…¥å¤šä¸ªå­—å¹•æ¡ç›®
                for part_text, (part_start, part_end) in zip(parts, times):
                    start = format_timestamp(part_start/0.7)
                    end = format_timestamp(part_end/0.7)

                    f.write(f"{index}\n")
                    f.write(f"{start} --> {end}\n")
                    f.write(f"{part_text}\n\n")
                    index += 1
            else:
                # æ— é€—å·ï¼Œä¿æŒåŸæ ·è¾“å‡º
                start = format_timestamp(segment.start)
                end = format_timestamp(segment.end)

                f.write(f"{index}\n")
                f.write(f"{start} --> {end}\n")
                f.write(f"{text}\n\n")
                index += 1

    print(f"âœ… SRT å­—å¹•æ–‡ä»¶å·²ç”Ÿæˆï¼š{output_srt_path}")
